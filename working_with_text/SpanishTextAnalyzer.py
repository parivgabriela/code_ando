import re
import math
from typing import Dict, Union
from pathlib import Path
from SpanishTextAnalyzerLibrary import count_spanish_sentences

class SpanishTextAnalyzer:
    """
    Analizador de texto en espa√±ol para calcular tokens y estad√≠sticas
    """
    
    def __init__(self):
        # Factores de conversi√≥n aproximados para espa√±ol
        self.CHARS_PER_TOKEN_ES = 4.5  # Promedio para espa√±ol (m√°s alto que ingl√©s)
        self.WORDS_PER_TOKEN_ES = 0.75  # Espa√±ol tiene palabras m√°s largas
        self.CHARS_PER_PAGE = 2000     # Caracteres por p√°gina est√°ndar
        self.WORDS_PER_PAGE = 400      # Palabras por p√°gina est√°ndar
        
        # Patrones espec√≠ficos del espa√±ol
        self.spanish_patterns = {
            'contractions': r"\b(del|al|conmigo|contigo|consigo)\b",
            'compound_words': r"\b\w+[-]\w+\b",
            'special_chars': r"[√±√°√©√≠√≥√∫√º¬ø¬°]",
            'punctuation': r"[.,;:!¬°?¬ø()\"'\-\[\]{}]"
        }
    
    def count_tokens_by_characters(self, text: str) -> int:
        """
        Estima tokens basado en conteo de caracteres (m√©todo m√°s simple)
        """
        # Limpiar texto pero conservar espacios significativos
        clean_text = re.sub(r'\s+', ' ', text.strip())
        char_count = len(clean_text)
        
        # Ajuste para espa√±ol - caracteres especiales cuentan m√°s
        spanish_chars = len(re.findall(self.spanish_patterns['special_chars'], text, re.IGNORECASE))
        adjusted_chars = char_count + (spanish_chars * 0.2)  # Ligero ajuste
        
        return max(1, math.ceil(adjusted_chars / self.CHARS_PER_TOKEN_ES))
    
    def count_tokens_by_words(self, text: str) -> int:
        """
        Estima tokens basado en conteo de palabras (m√°s preciso para espa√±ol)
        """
        # Tokenizar palabras considerando patrones espa√±oles
        words = re.findall(r'\b[a-z√°√©√≠√≥√∫√º√±]+\b', text.lower())
        
        # Contar contracciones y palabras compuestas
        contractions = len(re.findall(self.spanish_patterns['contractions'], text, re.IGNORECASE))
        compound_words = len(re.findall(self.spanish_patterns['compound_words'], text))
        
        # Palabras base
        base_words = len(words)
        
        # Ajuste para caracter√≠sticas del espa√±ol
        total_word_units = base_words + (contractions * 0.5) + compound_words
        
        return max(1, math.ceil(total_word_units / self.WORDS_PER_TOKEN_ES))
    
    def count_tokens_advanced(self, text: str) -> int:
        """
        M√©todo avanzado que combina m√∫ltiples heur√≠sticas para espa√±ol
        """
        # Limpieza inicial
        clean_text = text.strip()
        
        if not clean_text:
            return 0
        
        # M√∫ltiples estimaciones
        char_tokens = self.count_tokens_by_characters(clean_text)
        word_tokens = self.count_tokens_by_words(clean_text)
        
        # Factores de ajuste para espa√±ol
        punctuation_count = len(re.findall(self.spanish_patterns['punctuation'], text))
        punctuation_tokens = math.ceil(punctuation_count / 4)  # Puntuaci√≥n tambi√©n cuenta
        
        # Promedio ponderado favoreciendo el m√©todo de palabras para espa√±ol
        estimated_tokens = math.ceil(
            (word_tokens * 0.7) + 
            (char_tokens * 0.3) + 
            punctuation_tokens
        )
        
        return max(1, estimated_tokens)
    
    def analyze_text_content(self, text: str) -> Dict:
        """
        Analiza el contenido de un texto y devuelve estad√≠sticas completas
        """
        if not text:
            return self._empty_stats()
        
        # Estad√≠sticas b√°sicas
        char_count = len(text)
        char_count_no_spaces = len(re.sub(r'\s', '', text))
        
        # Conteo de palabras (m√©todo mejorado para espa√±ol)
        words = re.findall(r'\b[a-z√°√©√≠√≥√∫√º√±]+\b', text, re.IGNORECASE)
        word_count = len(words)
        
        # Conteo de oraciones
        sentences = count_spanish_sentences(text)
        sentence_count = len([s for s in sentences if s.strip()])
        
        # Conteo de p√°rrafos
        paragraphs = [p for p in text.split('\n\n') if p.strip()]
        paragraph_count = len(paragraphs)
        
        # Estimaci√≥n de p√°ginas
        pages_by_chars = math.ceil(char_count / self.CHARS_PER_PAGE)
        pages_by_words = math.ceil(word_count / self.WORDS_PER_PAGE)
        estimated_pages = max(pages_by_chars, pages_by_words)
        
        # C√°lculo de tokens con diferentes m√©todos
        tokens_simple = self.count_tokens_by_characters(text)
        tokens_word_based = self.count_tokens_by_words(text)
        tokens_advanced = self.count_tokens_advanced(text)
        
        # Estad√≠sticas adicionales
        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0
        avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0
        
        return {
            'characters': char_count,
            'characters_no_spaces': char_count_no_spaces,
            'words': word_count,
            'sentences': sentence_count,
            'paragraphs': paragraph_count,
            'estimated_pages': estimated_pages,
            'tokens': {
                'simple_estimate': tokens_simple,
                'word_based_estimate': tokens_word_based,
                'advanced_estimate': tokens_advanced,
                'recommended': tokens_advanced  # El m√°s preciso para espa√±ol
            },
            'ratios': {
                'chars_per_token': round(char_count / tokens_advanced, 2) if tokens_advanced > 0 else 0,
                'words_per_token': round(word_count / tokens_advanced, 2) if tokens_advanced > 0 else 0,
                'tokens_per_page': round(tokens_advanced / estimated_pages, 2) if estimated_pages > 0 else 0
            },
            'language_stats': {
                'avg_word_length': round(avg_word_length, 2),
                'avg_sentence_length': round(avg_sentence_length, 2),
                'spanish_chars': len(re.findall(r'[√±√°√©√≠√≥√∫√º¬ø¬°]', text, re.IGNORECASE))
            }
        }
    
    def analyze_file(self, file_path: Union[str, Path]) -> Dict:
        """
        Analiza un archivo de texto y devuelve estad√≠sticas completas
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"El archivo {file_path} no existe")
        
        if not file_path.is_file():
            raise ValueError(f"{file_path} no es un archivo")
        
        # Informaci√≥n del archivo
        file_info = {
            'file_path': str(file_path.absolute()),
            'file_name': file_path.name,
            'file_size_bytes': file_path.stat().st_size,
            'file_extension': file_path.suffix
        }
        
        # Leer archivo con encoding apropiado
        try:
            encodings = ['utf-8', 'latin-1', 'cp1252']
            text_content = None
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as file:
                        text_content = file.read()
                    break
                except UnicodeDecodeError:
                    continue
            
            if text_content is None:
                raise ValueError("No se pudo leer el archivo con ninguna codificaci√≥n")
            
        except Exception as e:
            raise ValueError(f"Error al leer el archivo: {str(e)}")
        
        # Analizar contenido
        content_stats = self.analyze_text_content(text_content)
        
        # Combinar informaci√≥n
        result = {**file_info, **content_stats}
        
        # Agregar m√©tricas espec√≠ficas del archivo
        result['file_metrics'] = {
            'kb_size': round(file_info['file_size_bytes'] / 1024, 2),
            'compression_ratio': round(content_stats['characters'] / file_info['file_size_bytes'], 2) if file_info['file_size_bytes'] > 0 else 0
        }
        
        return result
    
    def _empty_stats(self) -> Dict:
        """Devuelve estad√≠sticas vac√≠as"""
        return {
            'characters': 0, 'characters_no_spaces': 0, 'words': 0,
            'sentences': 0, 'paragraphs': 0, 'estimated_pages': 0,
            'tokens': {'simple_estimate': 0, 'word_based_estimate': 0, 
                      'advanced_estimate': 0, 'recommended': 0},
            'ratios': {'chars_per_token': 0, 'words_per_token': 0, 'tokens_per_page': 0},
            'language_stats': {'avg_word_length': 0, 'avg_sentence_length': 0, 'spanish_chars': 0}
        }

# Funciones de conveniencia
def analyze_spanish_text(text: str) -> Dict:
    """
    Funci√≥n simple para analizar texto espa√±ol
    
    Args:
        text: Texto en espa√±ol a analizar
    
    Returns:
        Diccionario con estad√≠sticas completas del texto
    """
    analyzer = SpanishTextAnalyzer()
    return analyzer.analyze_text_content(text)

def analyze_text_file(file_path: str) -> Dict:
    """
    Funci√≥n simple para analizar archivo de texto
    
    Args:
        file_path: Ruta al archivo de texto
    
    Returns:
        Diccionario con estad√≠sticas completas del archivo
    """
    analyzer = SpanishTextAnalyzer()
    return analyzer.analyze_file(file_path)

def get_token_count(text: str) -> int:
    """
    Funci√≥n r√°pida para obtener solo el conteo de tokens
    
    Args:
        text: Texto en espa√±ol
    
    Returns:
        N√∫mero estimado de tokens
    """
    analyzer = SpanishTextAnalyzer()
    return analyzer.count_tokens_advanced(text)

def print_analysis_report(stats: Dict, include_file_info: bool = True):
    """
    Imprime un reporte formateado de las estad√≠sticas
    """
    print("=" * 60)
    print("AN√ÅLISIS DE TEXTO EN ESPA√ëOL")
    print("=" * 60)
    
    if include_file_info and 'file_name' in stats:
        print(f"\nüìÅ INFORMACI√ìN DEL ARCHIVO:")
        print(f"   Nombre: {stats['file_name']}")
        print(f"   Tama√±o: {stats['file_metrics']['kb_size']} KB")
        print(f"   Ruta: {stats['file_path']}")
    
    print(f"\nüìä ESTAD√çSTICAS B√ÅSICAS:")
    print(f"   Caracteres: {stats['characters']:,}")
    print(f"   Caracteres (sin espacios): {stats['characters_no_spaces']:,}")
    print(f"   Palabras: {stats['words']:,}")
    print(f"   Oraciones: {stats['sentences']:,}")
    print(f"   P√°rrafos: {stats['paragraphs']:,}")
    print(f"   P√°ginas estimadas: {stats['estimated_pages']:,}")
    
    print(f"\nüî¢ ESTIMACI√ìN DE TOKENS:")
    print(f"   M√©todo simple: {stats['tokens']['simple_estimate']:,}")
    print(f"   M√©todo por palabras: {stats['tokens']['word_based_estimate']:,}")
    print(f"   M√©todo avanzado: {stats['tokens']['advanced_estimate']:,}")
    print(f"   ‚Üí RECOMENDADO: {stats['tokens']['recommended']:,} tokens")
    
    print(f"\nüìê PROPORCIONES:")
    print(f"   Caracteres por token: {stats['ratios']['chars_per_token']}")
    print(f"   Palabras por token: {stats['ratios']['words_per_token']}")
    print(f"   Tokens por p√°gina: {stats['ratios']['tokens_per_page']}")
    
    print(f"\nüá™üá∏ CARACTER√çSTICAS DEL ESPA√ëOL:")
    print(f"   Longitud promedio de palabra: {stats['language_stats']['avg_word_length']}")
    print(f"   Palabras promedio por oraci√≥n: {stats['language_stats']['avg_sentence_length']}")
    print(f"   Caracteres especiales del espa√±ol: {stats['language_stats']['spanish_chars']}")
    
    print("=" * 60)

# Ejemplo de uso
if __name__ == "__main__":
    # Ejemplo con texto directo
    sample_text = """
    La inteligencia artificial est√° transformando el mundo de manera acelerada. 
    Esta revoluci√≥n tecnol√≥gica presenta tanto oportunidades extraordinarias como 
    desaf√≠os √∫nicos para la humanidad. Los algoritmos de aprendizaje autom√°tico 
    pueden procesar informaci√≥n a velocidades impresionantes, pero tambi√©n plantean 
    preguntas importantes sobre √©tica y responsabilidad.
    
    En Espa√±a, las universidades est√°n incorporando estas tecnolog√≠as en sus 
    programas acad√©micos. Los estudiantes necesitan comprender no solo c√≥mo 
    funcionan estas herramientas, sino tambi√©n sus implicaciones sociales y √©ticas.
    """
    texto2 = """como calculo cuantos tokens tiene un texto en espa√±ol? Genera una funcion para reutilizar el codigo dado un input, la ruta del archivo txt me diga cantidad de paginas, cantidad de palabras, cantidad de caracteres y tokens"""
    texto_3 = """cual es el error AttributeError: module 'posixpath' has no attribute 'spbasedlitext' codigo output_path = os.path.spbasedlitext(filename)[0] + ".txt"""
    texto_4 = """Como estimo el gasto de un instituto que va a usar api de un modelo ia siendo que sale $5 input por millon de tokens y $10 output 1- como poner un limite para los usuarios. Lo recomendado 2- El usuario puede enviar archivos para el procesamiento maximo 10 paginas."""
    
    print("Ejemplo 1: An√°lisis de texto directo")
    stats = analyze_spanish_text(texto_4)
    print_analysis_report(stats, include_file_info=False)
    
    print("\n" + "="*60 + "\n")
    
    # Ejemplo con archivo (descomenta si tienes un archivo)
    # print("Ejemplo 2: An√°lisis de archivo")
    # try:
    #     path_files = "../archivos_prueba/"
    #     filename = "hoja-demo.txt"
    #     file_stats = analyze_text_file(path_files + filename)
    #     print_analysis_report(file_stats)
    # except FileNotFoundError:
    #     print("Archivo 'mi_documento.txt' no encontrado")
    
    # Funci√≥n r√°pida para obtener solo tokens
    print("Funci√≥n r√°pida - Solo tokens:")
    token_count = get_token_count(sample_text)
    print(f"Tokens estimados: {token_count}")